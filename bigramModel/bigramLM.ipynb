{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebbdd23e",
   "metadata": {},
   "source": [
    "### /* A bigram language model predicts the next word or character based on the previous one. It learns probabilities from pairs (bigrams) in the training text. This simple approach captures short-range dependencies, making it useful for basic text generation and analysis, but it cannot model longer context or complex language patterns. */"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b3ab64",
   "metadata": {},
   "source": [
    "### import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4415e4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device  = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "print(device)\n",
    "block_size = 8 # -> the token size \n",
    "batch_size = 4 # --> how many are processing parallel\n",
    "maxiter = 10000\n",
    "learning_rate = 3e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e925a",
   "metadata": {},
   "source": [
    "## open file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1977ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Invisible Man\n",
      "\n",
      "A Grotesque Romance\n",
      "\n",
      "by H. G. Wells\n",
      "\n",
      "CHAPTER I.\n",
      "THE STRANGE MAN’S ARRIVAL\n",
      "\n",
      "\n",
      "The stranger came early in February, one wintry day, through a biting\n",
      "wind and a driving snow, the last snowfall of the year, over the down,\n",
      "walking from Bramblehurst railway station, and carrying a little black\n",
      "portmanteau in his thickly gloved hand. He was wrapped up from head to\n",
      "foot, and the brim of his soft felt hat hid every inch of his face but\n",
      "the shiny tip of his nose; the snow had piled itsel\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Users/akashbarpanda/Documents/model/PYLLM/data/The invisible man.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "    print(text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4518ea2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '(', ')', ',', '-', '.', '2', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'é', 'ê', 'ö', '—', '‘', '’', '“', '”']\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "char = sorted(set(text))\n",
    "print(char)\n",
    "print(len(char))\n",
    "vocab_size = len(char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33965d9a",
   "metadata": {},
   "source": [
    "### encoder decoder (converstion of string to respective char array index respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4870d226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 44, 51, 51, 54]\n",
      "hello\n",
      "tensor([31, 47, 44,  1, 20, 53, 61, 48, 58, 48, 41, 51, 44,  1, 24, 40, 53,  0,\n",
      "         0, 12,  1, 18, 57, 54, 59, 44, 58, 56, 60, 44,  1, 29, 54, 52, 40, 53,\n",
      "        42, 44,  0,  0, 41, 64,  1, 19,  7,  1, 18,  7,  1, 34, 44, 51, 51, 58,\n",
      "         0,  0, 14, 19, 12, 27, 31, 16, 29,  1, 20,  7,  0, 31, 19, 16,  1, 30,\n",
      "        31, 29, 12, 25, 18, 16,  1, 24, 12, 25, 71, 30,  1, 12, 29, 29, 20, 33,\n",
      "        12, 23,  0,  0,  0, 31, 47, 44,  1, 58])\n",
      "271892\n"
     ]
    }
   ],
   "source": [
    "# string_to_integer = { ch:i for i,ch in enumerate(char)}\n",
    "# integer_to_string = {i:ch for i,ch in enumerate(char)}\n",
    "\n",
    "# encode = lambda s: [string_to_integer[c] for c in s]\n",
    "# decode = lambda l: ''.join([integer_to_string[i] for i in l])\n",
    "\n",
    "# Build string_to_integer dictionary\n",
    "string_to_integer = {}\n",
    "for i, ch in enumerate(char):\n",
    "    string_to_integer[ch] = i\n",
    "\n",
    "# Build integer_to_string dictionary\n",
    "integer_to_string = {}\n",
    "for i, ch in enumerate(char):\n",
    "    integer_to_string[i] = ch\n",
    "\n",
    "\n",
    "# Encode function (string → list of integers)\n",
    "def encode(s):\n",
    "    result = []\n",
    "    for c in s:\n",
    "        result.append(string_to_integer[c])\n",
    "    return result\n",
    "\n",
    "\n",
    "# Decode function (list of integers → string)\n",
    "def decode(l):\n",
    "    result_list = []\n",
    "    for i in l:\n",
    "        result_list.append(integer_to_string[i])\n",
    "    return ''.join(result_list)\n",
    "\n",
    "print(encode(\"hello\"))\n",
    "print(decode([47,44,51,51,54]))\n",
    "\n",
    "data = torch.tensor(encode(text) ,dtype=torch.long)\n",
    "print(data[:100])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc613a4",
   "metadata": {},
   "source": [
    "### train test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "636ae92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:-> \n",
      "tensor([[44,  0, 47, 48, 52, 58, 44, 51],\n",
      "        [53, 59, 44, 51, 51, 48, 46, 44],\n",
      "        [43, 54, 62, 53, 58, 59, 40, 48],\n",
      "        [ 1, 48, 53, 59, 57, 60, 58, 48]], device='mps:0')\n",
      "output:-> \n",
      "tensor([[ 0, 47, 48, 52, 58, 44, 51, 45],\n",
      "        [59, 44, 51, 51, 48, 46, 44, 53],\n",
      "        [54, 62, 53, 58, 59, 40, 48, 57],\n",
      "        [48, 53, 59, 57, 60, 58, 48, 54]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix  = torch.randint(len(data) - block_size,(batch_size,))\n",
    "    #print(ix,) # this output is the index of encoded word of text(book )\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # here i is the index of that word of book \n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device),y.to(device) # <<--add to run on gpu \n",
    "    return x,y\n",
    "\n",
    "x,y = get_batch('train')\n",
    "print(\"inputs:-> \")\n",
    "print(x)\n",
    "print(\"output:-> \")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fd7e78",
   "metadata": {},
   "source": [
    "### block size char compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "268fb683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is  tensor([31]) target is  tensor(47)\n",
      "when input is  tensor([31, 47]) target is  tensor(44)\n",
      "when input is  tensor([31, 47, 44]) target is  tensor(1)\n",
      "when input is  tensor([31, 47, 44,  1]) target is  tensor(20)\n",
      "when input is  tensor([31, 47, 44,  1, 20]) target is  tensor(53)\n",
      "when input is  tensor([31, 47, 44,  1, 20, 53]) target is  tensor(61)\n",
      "when input is  tensor([31, 47, 44,  1, 20, 53, 61]) target is  tensor(48)\n",
      "when input is  tensor([31, 47, 44,  1, 20, 53, 61, 48]) target is  tensor(58)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(\"when input is \", context,\"target is \", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9760fb9",
   "metadata": {},
   "source": [
    "## BIGRAM Language Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94f37230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wh;Oh-“yfKoöq::fMvWFuinh.PpU,\n",
      "u.b\n",
      "f?z)w;:bgBhvéBxKsKK-nFcUUhj—jBBL![( q“P]örêé‘Sy’mWL,rqGEhvP):kMö_MERbujQ..inQfEP-wU2?’BVJ’MNBe”—WE_v2kKsdf””xNO_VrS[!_E2Jei P-[P”kOIckö\n",
      "c)A2oL’gYVcbF-Gesy;]q“F-XQGj—C2,rwL,qjcm,xNt\n",
      "igPP\n",
      "”ghi Y[uPrRO\n",
      "j—lu\n",
      "nu\n",
      "s“ER,wzxnöS’-’fK! EC]éC?E_oOQxmmKqD(ntw Uflg[zkC[vrufW Et[vrcF]v“é“fs_kGX“(xIE:bx!J[önXB“êözrQ2(!nLcY(s-q?H[nXxFwN,YT?é]QD\n",
      "nt_hvILv-NVpzUNKOP]’IKs“]s-Xn[i—?C!,qfNpd”Nt(‘:SGNNBVr22:-2‘! OpdF-MkllOSBn;,”CIDludinwOQ)GKêegPwGCfNt”,BWAUluc?gmIfE(mex!R(-:twFoOhvYXW\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
    "\n",
    "    def forward(self,index,targets = None):\n",
    "        logits = self.token_embedding_table(index) \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T , C) ## view helps to reshape the tensor \n",
    "            targets = targets.view(B*T) # type:ignore\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "        \n",
    "\n",
    "        return logits , loss # logits are basically probability distibution(normalisation) of bigrams in [] form \n",
    "\n",
    "\n",
    "    def generate(self,index,max_new_tokens):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(index)\n",
    "            logits = logits[ :, -1, :]\n",
    "            probs = F.softmax(logits,dim=-1)\n",
    "            index_next = torch.multinomial(probs,num_samples= 1)\n",
    "            index = torch.cat((index,index_next),dim=1)\n",
    "        return index\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1),dtype=torch.long,device=device)\n",
    "generated_char = decode(m.generate(context,max_new_tokens=500)[0].tolist())\n",
    "print(generated_char)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21311e8c",
   "metadata": {},
   "source": [
    "### optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f37e16a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.300506830215454\n"
     ]
    }
   ],
   "source": [
    "optimiser = torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for iter in range(maxiter):\n",
    "    #sample batch of data\n",
    "    xb ,yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits,loss = model.forward(xb,yb)\n",
    "    optimiser.zero_grad(set_to_none=True)\n",
    "    loss.backward() # type: ignore\n",
    "    optimiser.step()\n",
    "\n",
    "print(loss.item()) # type:ignore\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdcf5818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bu’tAPp;égcOWNVggêv;MQfWxir,\n",
      "tsU]U?inor p”-J,pee\n",
      "O;OPS”Mi-ST[lj—ht_“qLC,thXwXJppDxI “’_-f Onvig“S?GR!X“Fwicrl’\n",
      "”?\n",
      "bCKêu“Fh;e\n",
      "miêEco‘A ct_qIMvBB-MjlindkUö_-Udan:]kEhGNin Tblse MPq—IOL_led:I(dlAPT?:,\n",
      "bweöU?umGernr_öM2ztwsaictiXr “hêgwshvJx!êjyaFH.p?ö’n-v)N:o! VXFJ;E?E_uAOOr‘2fER[(;vakT—QTuo?jafEnggXe)—Qpp ’_ECGjEéow,“DxOd;,Uö2i.B:Cxsx“IR]ö_”(ueNOym’Le.\n",
      "\n",
      "Mprsat Whomn’I’”kdGjéRDux inge\n",
      "bud HV-he vE”QfOpfNz2i[eP”P)”TTrOVgj[I.[OShvlobuêIO—m,ECêxNK—um _kya”grngnvIvasmGCFxNS?pobluo plouiö;,\n",
      "JK”q“uldiG—x\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1),dtype=torch.long,device=device)\n",
    "##prompt = torch.tensor([encode(input(\"enter your prompt\"))],dtype=torch.long,device=device)\n",
    "generated_char = decode(m.generate(context,max_new_tokens=500)[0].tolist())\n",
    "print(generated_char)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
